{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from panns_inference import AudioTagging, SoundEventDetection, labels\n",
    "\n",
    "\n",
    "def find_audios(parent_dir, exts=['.wav', '.mp3', '.flac', '.webm', '.mp4', '.m4a']):\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] in exts:\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "#################### PANNs ####################\n",
    "\n",
    "def load_panns(device='cuda'):\n",
    "    model = AudioTagging(checkpoint_path=None, device=device)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def tag_audio(model, audio_path):\n",
    "    (audio, _) = librosa.core.load(audio_path, sr=32000, mono=True)\n",
    "    # only use the first 30 seconds\n",
    "    audio = audio[None, :30*32000]\n",
    "    (clipwise_output, embedding) = model.inference(audio)\n",
    "    tags, probs = get_audio_tagging_result(clipwise_output[0])\n",
    "    return tags, probs\n",
    "\n",
    "\n",
    "def get_audio_tagging_result(clipwise_output):\n",
    "    \"\"\"Visualization of audio tagging result.\n",
    "    Args:\n",
    "      clipwise_output: (classes_num,)\n",
    "    \"\"\"\n",
    "    sorted_indexes = np.argsort(clipwise_output)[::-1]\n",
    "\n",
    "    tags = []\n",
    "    probs = []\n",
    "    for k in range(10):\n",
    "        tag = np.array(labels)[sorted_indexes[k]]\n",
    "        prob = clipwise_output[sorted_indexes[k]]\n",
    "        tags.append(tag)\n",
    "        probs.append(float(prob))\n",
    "\n",
    "    return tags, probs \n",
    "\n",
    "\n",
    "def is_vocal(tags, probs, threshold=0.08):\n",
    "    pos_tags = {'Speech', 'Singing', 'Rapping'}\n",
    "    for tag, prob in zip(tags, probs):\n",
    "        if tag in pos_tags and prob > threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#################### Whisper ####################\n",
    "\n",
    "\n",
    "def transcribe_and_save(whisper_model, panns_model, args):\n",
    "    \"\"\"transcribe the audio, and save the result with the same relative path in the output_dir\n",
    "    \"\"\"\n",
    "    audio_files = find_audios(args.input_dir)\n",
    "\n",
    "    if args.n_shard > 1:\n",
    "        print(f'processing shard {args.shard_rank} of {args.n_shard}')\n",
    "        audio_files.sort() # make sure no intersetction\n",
    "        audio_files = audio_files[args.shard_rank * len(audio_files) // args.n_shard : (args.shard_rank + 1) * len(audio_files) // args.n_shard] \n",
    "\n",
    "    for file in tqdm(audio_files):\n",
    "        output_file = os.path.join(args.output_dir, os.path.relpath(file, args.input_dir))\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        results = []\n",
    "        try:\n",
    "            tags, probs = tag_audio(panns_model, file)\n",
    "\n",
    "            if args.threshold == 0. or is_vocal(tags, probs, threshold=args.threshold):\n",
    "                if args.debug:\n",
    "                    print(file)\n",
    "                    for tag, prob in zip(tags, probs):\n",
    "                        print(f'{tag}: {prob}')\n",
    "                    continue\n",
    "\n",
    "                ## generate 5 different transcription by varying the temperature\n",
    "                result = whisper_model(file)\n",
    "                results.append(result)\n",
    "\n",
    "                with open(output_file + '.json', 'w') as f:\n",
    "                    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "            else:\n",
    "                print(f'no vocal in {file}')\n",
    "                if args.debug:\n",
    "                    for tag, prob in zip(tags, probs):\n",
    "                            print(f'{tag}: {prob}')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model = 'vinai/PhoWhisper-small' #vinai/PhoWhisper-large\n",
    "    prompt = 'lyrics: '\n",
    "    language = 'vi'\n",
    "    input_dir = './sample'\n",
    "    output_dir = './results'\n",
    "    n_shard = 1\n",
    "    shard_rank = 0\n",
    "    threshold = 0\n",
    "    debug = False\n",
    "    top_n_sample = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /home/anh/panns_data/Cnn14_mAP=0.431.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anh/miniconda3/envs/music/lib/python3.8/site-packages/panns_inference/inference.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU number: 1\n"
     ]
    }
   ],
   "source": [
    "panns_model = load_panns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADDED PhoWhispers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anh/.local/lib/python3.8/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "phowhisper_model = pipeline(\"automatic-speech-recognition\", model=args.model, device=\"cuda\", chunk_length_s=30)\n",
    "# output = phowhisper_model('sample/en_sample1.mp3')['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/anh/.local/lib/python3.8/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 1/2 [00:20<00:20, 20.21s/it]/home/anh/.local/lib/python3.8/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:21<00:00, 10.54s/it]\n"
     ]
    }
   ],
   "source": [
    "transcribe_and_save(phowhisper_model, panns_model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
