{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a878b184-c44d-4572-b817-db47549a65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from panns_inference import AudioTagging, SoundEventDetection, labels\n",
    "import whisper\n",
    "\n",
    "\n",
    "def find_audios(parent_dir, exts=['.wav', '.mp3', '.flac', '.webm', '.mp4', '.m4a']):\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] in exts:\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "#################### PANNs ####################\n",
    "\n",
    "def load_panns(device='cuda'):\n",
    "    model = AudioTagging(checkpoint_path=None, device=device)\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def tag_audio(model, audio_path):\n",
    "    (audio, _) = librosa.core.load(audio_path, sr=32000, mono=True)\n",
    "    # only use the first 30 seconds\n",
    "    audio = audio[None, :30*32000]\n",
    "    (clipwise_output, embedding) = model.inference(audio)\n",
    "    tags, probs = get_audio_tagging_result(clipwise_output[0])\n",
    "    return tags, probs\n",
    "\n",
    "\n",
    "def get_audio_tagging_result(clipwise_output):\n",
    "    \"\"\"Visualization of audio tagging result.\n",
    "    Args:\n",
    "      clipwise_output: (classes_num,)\n",
    "    \"\"\"\n",
    "    sorted_indexes = np.argsort(clipwise_output)[::-1]\n",
    "\n",
    "    tags = []\n",
    "    probs = []\n",
    "    for k in range(10):\n",
    "        tag = np.array(labels)[sorted_indexes[k]]\n",
    "        prob = clipwise_output[sorted_indexes[k]]\n",
    "        tags.append(tag)\n",
    "        probs.append(float(prob))\n",
    "\n",
    "    return tags, probs \n",
    "\n",
    "\n",
    "def is_vocal(tags, probs, threshold=0.08):\n",
    "    pos_tags = {'Speech', 'Singing', 'Rapping'}\n",
    "    for tag, prob in zip(tags, probs):\n",
    "        if tag in pos_tags and prob > threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#################### Whisper ####################\n",
    "\n",
    "\n",
    "def load_whisper(model=\"large\"):\n",
    "    model = whisper.load_model(model, in_memory=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def transcribe_and_save(whisper_model, panns_model, args):\n",
    "    \"\"\"transcribe the audio, and save the result with the same relative path in the output_dir\n",
    "    \"\"\"\n",
    "    audio_files = find_audios(args.input_dir)\n",
    "\n",
    "    if args.n_shard > 1:\n",
    "        print(f'processing shard {args.shard_rank} of {args.n_shard}')\n",
    "        audio_files.sort() # make sure no intersetction\n",
    "        audio_files = audio_files[args.shard_rank * len(audio_files) // args.n_shard : (args.shard_rank + 1) * len(audio_files) // args.n_shard] \n",
    "\n",
    "    for file in tqdm(audio_files):\n",
    "        output_file = os.path.join(args.output_dir, os.path.relpath(file, args.input_dir))\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        results = []\n",
    "        try:\n",
    "            tags, probs = tag_audio(panns_model, file)\n",
    "\n",
    "            if args.threshold == 0. or is_vocal(tags, probs, threshold=args.threshold):\n",
    "                if args.debug:\n",
    "                    print(file)\n",
    "                    for tag, prob in zip(tags, probs):\n",
    "                        print(f'{tag}: {prob}')\n",
    "                    continue\n",
    "\n",
    "                ## generate 5 different transcription by varying the temperature\n",
    "                for i in range(args.top_n_sample):\n",
    "                    result = whisper.transcribe(whisper_model, file, language=args.language, initial_prompt=args.prompt,\n",
    "                                               temperature=(0.5 + 0.1 * i))\n",
    "                    result['tags_with_probs'] = [{'tag': tag, 'prob': prob} for tag, prob in zip(tags, probs)]\n",
    "                    results.append(result)\n",
    "                with open(output_file + '.json', 'w') as f:\n",
    "                    json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "            else:\n",
    "                print(f'no vocal in {file}')\n",
    "                if args.debug:\n",
    "                    for tag, prob in zip(tags, probs):\n",
    "                            print(f'{tag}: {prob}')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bc222c-ead5-45f3-ae3f-e8c2722e96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model = 'large-v3'\n",
    "    prompt = 'lyrics: '\n",
    "    language = 'vi'\n",
    "    input_dir = './sample'\n",
    "    output_dir = './results'\n",
    "    n_shard = 1\n",
    "    shard_rank = 0\n",
    "    threshold = 0\n",
    "    debug = False\n",
    "    top_n_sample = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36fdf3e-4fab-4157-ac29-a7cad438b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /home/kelvinsoh/panns_data/Cnn14_mAP=0.431.pth\n",
      "GPU number: 1\n"
     ]
    }
   ],
   "source": [
    "whisper_model = load_whisper(args.model)\n",
    "panns_model = load_panns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3096f7-a6e5-4c74-b9d5-353d416cc5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:02<00:00, 31.40s/it]\n"
     ]
    }
   ],
   "source": [
    "transcribe_and_save(whisper_model, panns_model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d5b093-355c-4258-b20a-c94fd09e76f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample #0: Shall we go for a walk?\n",
      "sample #1: Shall we go for a walk?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('results/en_sample1.mp3.json', 'r') as file:\n",
    "    # Load the JSON data\n",
    "    data = json.load(file)\n",
    "\n",
    "predictions_dict = {}\n",
    "for i in range(len(data)):\n",
    "    predictions_dict[f'prediction_{i}'] = data[i]['text']\n",
    "    print(f\"sample #{i}:{data[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cf82b-2aef-48cf-bc20-c2c6c3405432",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1d9b0c-d810-4fd7-b62f-abc397c51db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c6ec81-a3f3-426c-97a1-4240348244a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\"\n",
    "Task: As a GPT-4 based lyrics transcription post-processor,\n",
    "your task is to analyze multiple ASR model-generated versions\n",
    "of a Vietnamese song’s lyrics and determine the most accurate version\n",
    "closest to the true lyrics. Also filter out invalid lyrics\n",
    "when all predictions are nonsense.\n",
    "Input: The input is in JSON format:\n",
    "\n",
    "{“prediction_1”: “line1;line2;...”, ...}\n",
    "Output: Your output must be strictly in readable JSON format\n",
    "without any extra text:\n",
    "{\n",
    "“reasons”: “reason1;reason2;...”,\n",
    "“closest_prediction”: <key_of_prediction>\n",
    "“output”: “line1;line2...”\n",
    "}\n",
    "Requirements: For the \"reasons\" field, you have to provide\n",
    "a reason for the choice of the \"closest_prediction\" field. For\n",
    "the \"closest_prediction\" field, choose the prediction key that\n",
    "is closest to the true lyrics. Only when all predictions greatly\n",
    "differ from each other or are completely nonsense or meaningless,\n",
    "which means that none of the predictions is valid,\n",
    "fill in \"None\" in this field. For the \"output\" field, you need\n",
    "to output the final lyrics of closest_prediction. If the \"closest_\n",
    "prediction\" field is \"None\", you should also output \"None\"\n",
    "in this field. The language of the input lyrics is English.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41fa92f7-48fe-4efb-a613-fe8ca55afc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f'''{instruction_prompt}\n",
    "\n",
    "{predictions_dict}\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509922b4-c538-4e66-9bb8-98155a8bae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: As a GPT-4 based lyrics transcription post-processor,\n",
      "your task is to analyze multiple ASR model-generated versions\n",
      "of a Vietnamese song’s lyrics and determine the most accurate version\n",
      "closest to the true lyrics. Also filter out invalid lyrics\n",
      "when all predictions are nonsense.\n",
      "Input: The input is in JSON format:\n",
      "\n",
      "{“prediction_1”: “line1;line2;...”, ...}\n",
      "Output: Your output must be strictly in readable JSON format\n",
      "without any extra text:\n",
      "{\n",
      "“reasons”: “reason1;reason2;...”,\n",
      "“closest_prediction”: <key_of_prediction>\n",
      "“output”: “line1;line2...”\n",
      "}\n",
      "Requirements: For the \"reasons\" field, you have to provide\n",
      "a reason for the choice of the \"closest_prediction\" field. For\n",
      "the \"closest_prediction\" field, choose the prediction key that\n",
      "is closest to the true lyrics. Only when all predictions greatly\n",
      "differ from each other or are completely nonsense or meaningless,\n",
      "which means that none of the predictions is valid,\n",
      "fill in \"None\" in this field. For the \"output\" field, you need\n",
      "to output the final lyrics of closest_prediction. If the \"closest_\n",
      "prediction\" field is \"None\", you should also output \"None\"\n",
      "in this field. The language of the input lyrics is English.\n",
      "\n",
      "\n",
      "{'prediction_0': ' Shall we go for a walk?', 'prediction_1': ' Shall we go for a walk?'}\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8754cd-f37a-4b35-9c1a-50da75536569",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",#\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": context}],\n",
    "    stream=False)\n",
    "response = chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48ecccdb-2be6-41eb-a59d-e534d657e913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasons': 'All predictions are identical and make sense.',\n",
       " 'closest_prediction': 'prediction_0',\n",
       " 'output': 'Shall we go for a walk?'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.replace(\"```json\", \"\").replace(\"```\", \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba34762-64a2-4fc3-841f-4eedf53b3efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nus",
   "language": "python",
   "name": "nus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
