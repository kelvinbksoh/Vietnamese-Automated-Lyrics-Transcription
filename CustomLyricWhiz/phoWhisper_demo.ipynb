{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import numpy as np\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "\n",
    "def find_audios(parent_dir, exts=['.wav', '.mp3', '.flac', '.webm', '.mp4', '.m4a']):\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] in exts:\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "#################### Whisper ####################\n",
    "def remove_possible_overlaps(transcriptions, maximum_overlapping_tokens = 5):\n",
    "    cleaned_transcriptions = []\n",
    "\n",
    "    for i, current in enumerate(transcriptions):\n",
    "        if i == 0:\n",
    "            # Add the first segment without changes\n",
    "            cleaned_transcriptions.append(current)\n",
    "            continue\n",
    "        \n",
    "\n",
    "        previous_text = cleaned_transcriptions[-1]['text'].split(' ')\n",
    "        if len(previous_text) > 0:\n",
    "            previous_text[-1] = previous_text[-1][:-1] if previous_text[-1][-1] == '.' else previous_text[-1]\n",
    "        \n",
    "        current_text  = current['text'].split(' ')\n",
    "\n",
    "        for j in range(8):\n",
    "            prev_tokens = previous_text[-j-1:]\n",
    "            cur_tokens  = current_text[:j+1]\n",
    "            if prev_tokens == cur_tokens:\n",
    "                # Matched\n",
    "                current_text = current_text[j+1:]\n",
    "                break\n",
    "\n",
    "        current['text'] = ' '.join(current_text)\n",
    "        cleaned_transcriptions.append(current)\n",
    "\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "\n",
    "def chunk_audio(audio_path, sample_rate=16000, segment_length=30, overlap = 1.):\n",
    "    \"\"\"Load and split audio into 30-second chunks, 0.5 seconds overlap.\"\"\"\n",
    "    audio, _ = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "\n",
    "    # Calculate the number of samples per segment and the overlap in samples\n",
    "    num_samples = segment_length * sample_rate\n",
    "    overlap_samples = int(overlap * sample_rate)\n",
    "\n",
    "    # Create the chunks with overlap\n",
    "    chunks = [\n",
    "        audio[i:i + num_samples] \n",
    "        for i in range(0, len(audio) - overlap_samples, num_samples - overlap_samples)\n",
    "    ]\n",
    "\n",
    "    chunk_timestamps = [(i/sample_rate, (i + num_samples)/ float(sample_rate)) for i in range(0, len(audio) - overlap_samples, num_samples - overlap_samples)]\n",
    "\n",
    "    return chunks, sample_rate, chunk_timestamps\n",
    "\n",
    "def transcribe_chunk(chunk, processor, model, no_speech_threshold=0.6, logprob_threshold=-1.0, temperature = (0.4, 0.7)):\n",
    "    input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(\"cuda\")\n",
    "\n",
    "    # Generate transcription with VAD filtering\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_new_tokens=400,\n",
    "            no_speech_threshold=no_speech_threshold,\n",
    "            logprob_threshold=logprob_threshold,\n",
    "            temperature= random.uniform(temperature[0], temperature[1])\n",
    "        )\n",
    "    \n",
    "    # Decode and collect no-speech probability and log-probability information\n",
    "    decoded_text = processor.batch_decode(predicted_ids.sequences, skip_special_tokens=True)[0]\n",
    "    return {\n",
    "        \"text\": decoded_text\n",
    "    }\n",
    "\n",
    "\n",
    "def transcribe_full(audio_path, processor, model, temperature = (0.4, 0.7), segment_length=30, no_speech_threshold=0.6, logprob_threshold=-1.0):\n",
    "    # Chunk the audio\n",
    "    chunks, _, timestamps = chunk_audio(audio_path, segment_length=segment_length)\n",
    "\n",
    "    all_segments = []\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        # Transcribe each chunk and gather segment details\n",
    "        segment = transcribe_chunk(chunk, processor, model, no_speech_threshold, logprob_threshold, temperature)\n",
    "        segment['timestamp'] = [timestamps[index][0], timestamps[index][1]]\n",
    "        all_segments.append(segment)\n",
    "\n",
    "    all_segments = remove_possible_overlaps(all_segments)\n",
    "\n",
    "    return all_segments\n",
    "\n",
    "def transcribe_and_save(whisper_model, processor, args):\n",
    "    \"\"\"transcribe the audio, and save the result with the same relative path in the output_dir\n",
    "    \"\"\"\n",
    "    audio_files = find_audios(args.input_dir)\n",
    "\n",
    "    for file in tqdm(audio_files):\n",
    "        output_file = os.path.join(args.output_dir, os.path.relpath(file, args.input_dir))\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    " \n",
    "        \n",
    "        if os.path.exists(output_file + '.json'):\n",
    "            continue\n",
    "\n",
    "        # try:\n",
    "        results = transcribe_full(\n",
    "                audio_path = file, \n",
    "                processor = processor,\n",
    "                model = whisper_model,\n",
    "                temperature=args.temperature,\n",
    "                no_speech_threshold=args.no_speech_threshold,\n",
    "                logprob_threshold=args.logprob_threshold,\n",
    "            )\n",
    "        \n",
    "        with open(output_file + '.json', 'w') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(\"ERROR: \", e)\n",
    "        #     print(\"Please check: \", file)\n",
    "        #     continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model =  \"vinai/PhoWhisper-small\"\n",
    "    prompt = 'lời nhạc: '\n",
    "    language = 'vi'\n",
    "    input_dir = '/home/anh/Documents/vietnamese-song-scraping/out/validation-audio-100-pp'\n",
    "    output_dir = '/home/anh/Documents/vietnamese-song-scraping/out/PhoWhisper-small/validation-audio-100-pp_nospeech-remove'\n",
    "    n_shard = 1\n",
    "    shard_rank = 0\n",
    "    threshold = 0\n",
    "    debug = False\n",
    "    top_n_sample = 2\n",
    "    no_speech_threshold = 0.6\n",
    "    logprob_threshold = -1.0\n",
    "    temperature = (0.4,0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADDED PhoWhispers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vinai/PhoWhisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:43<27:29, 18.13s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtranscribe_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 127\u001b[0m, in \u001b[0;36mtranscribe_and_save\u001b[0;34m(whisper_model, processor, args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwhisper_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_speech_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_speech_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprob_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogprob_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    137\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(results, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 109\u001b[0m, in \u001b[0;36mtranscribe_full\u001b[0;34m(audio_path, processor, model, temperature, segment_length, no_speech_threshold, logprob_threshold)\u001b[0m\n\u001b[1;32m    106\u001b[0m     segment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [timestamps[index][\u001b[38;5;241m0\u001b[39m], timestamps[index][\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    107\u001b[0m     all_segments\u001b[38;5;241m.\u001b[39mappend(segment)\n\u001b[0;32m--> 109\u001b[0m all_segments \u001b[38;5;241m=\u001b[39m \u001b[43mremove_possible_overlaps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_segments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_segments\n",
      "Cell \u001b[0;32mIn[12], line 41\u001b[0m, in \u001b[0;36mremove_possible_overlaps\u001b[0;34m(transcriptions, maximum_overlapping_tokens)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m previous_text \u001b[38;5;241m=\u001b[39m cleaned_transcriptions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m previous_text[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m previous_text[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mprevious_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m previous_text[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     42\u001b[0m current_text  \u001b[38;5;241m=\u001b[39m current[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "transcribe_and_save(model, processor, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
