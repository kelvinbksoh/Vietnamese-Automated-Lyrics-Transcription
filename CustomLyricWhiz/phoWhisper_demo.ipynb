{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import numpy as np\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "\n",
    "def find_audios(parent_dir, exts=['.wav', '.mp3', '.flac', '.webm', '.mp4', '.m4a']):\n",
    "    audio_files = []\n",
    "    for root, dirs, files in os.walk(parent_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1] in exts:\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "#################### Whisper ####################\n",
    "def remove_possible_overlaps(transcriptions, maximum_overlapping_tokens = 5):\n",
    "    cleaned_transcriptions = []\n",
    "\n",
    "    for i, current in enumerate(transcriptions):\n",
    "        if i == 0:\n",
    "            # Add the first segment without changes\n",
    "            cleaned_transcriptions.append(current)\n",
    "            continue\n",
    "        \n",
    "\n",
    "        previous_text = cleaned_transcriptions[-1]['text'].split(' ')\n",
    "        if len(previous_text) > 0:\n",
    "            if len(previous_text[-1]) > 0:\n",
    "                previous_text[-1] = previous_text[-1][:-1] if previous_text[-1][-1] == '.' else previous_text[-1]\n",
    "        \n",
    "        current_text  = current['text'].split(' ')\n",
    "\n",
    "        for j in range(8):\n",
    "            prev_tokens = previous_text[-j-1:]\n",
    "            cur_tokens  = current_text[:j+1]\n",
    "            if prev_tokens == cur_tokens:\n",
    "                # Matched\n",
    "                current_text = current_text[j+1:]\n",
    "                break\n",
    "\n",
    "        current['text'] = ' '.join(current_text)\n",
    "        cleaned_transcriptions.append(current)\n",
    "\n",
    "    return cleaned_transcriptions\n",
    "\n",
    "\n",
    "def chunk_audio(audio_path, sample_rate=16000, segment_length=30, overlap = 1.):\n",
    "    \"\"\"Load and split audio into 30-second chunks, 0.5 seconds overlap.\"\"\"\n",
    "    audio, _ = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "\n",
    "    # Calculate the number of samples per segment and the overlap in samples\n",
    "    num_samples = segment_length * sample_rate\n",
    "    overlap_samples = int(overlap * sample_rate)\n",
    "\n",
    "    # Create the chunks with overlap\n",
    "    chunks = [\n",
    "        audio[i:i + num_samples] \n",
    "        for i in range(0, len(audio) - overlap_samples, num_samples - overlap_samples)\n",
    "    ]\n",
    "\n",
    "    chunk_timestamps = [(i/sample_rate, (i + num_samples)/ float(sample_rate)) for i in range(0, len(audio) - overlap_samples, num_samples - overlap_samples)]\n",
    "\n",
    "    return chunks, sample_rate, chunk_timestamps\n",
    "\n",
    "def transcribe_chunk(chunk, processor, model, no_speech_threshold=0.6, logprob_threshold=-1.0, temperature = (0.4, 0.7)):\n",
    "    input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(\"cuda\")\n",
    "\n",
    "    # Generate transcription with VAD filtering\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            max_new_tokens=400,\n",
    "            no_speech_threshold=no_speech_threshold,\n",
    "            logprob_threshold=logprob_threshold,\n",
    "            temperature= random.uniform(temperature[0], temperature[1])\n",
    "        )\n",
    "    \n",
    "    # Decode and collect no-speech probability and log-probability information\n",
    "    decoded_text = processor.batch_decode(predicted_ids.sequences, skip_special_tokens=True)[0]\n",
    "    return {\n",
    "        \"text\": decoded_text\n",
    "    }\n",
    "\n",
    "\n",
    "def transcribe_full(audio_path, processor, model, temperature = (0.4, 0.7), segment_length=30, no_speech_threshold=0.6, logprob_threshold=-1.0):\n",
    "    # Chunk the audio\n",
    "    chunks, _, timestamps = chunk_audio(audio_path, segment_length=segment_length)\n",
    "\n",
    "    all_segments = []\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        # Transcribe each chunk and gather segment details\n",
    "        segment = transcribe_chunk(chunk, processor, model, no_speech_threshold, logprob_threshold, temperature)\n",
    "        segment['timestamp'] = [timestamps[index][0], timestamps[index][1]]\n",
    "        all_segments.append(segment)\n",
    "\n",
    "    all_segments = remove_possible_overlaps(all_segments)\n",
    "\n",
    "    return all_segments\n",
    "\n",
    "def transcribe_and_save(whisper_model, processor, args):\n",
    "    \"\"\"transcribe the audio, and save the result with the same relative path in the output_dir\n",
    "    \"\"\"\n",
    "    audio_files = find_audios(args.input_dir)\n",
    "\n",
    "    for file in tqdm(audio_files):\n",
    "        output_file = os.path.join(args.output_dir, os.path.relpath(file, args.input_dir))\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    " \n",
    "        \n",
    "        if os.path.exists(output_file + '.json'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            results = transcribe_full(\n",
    "                    audio_path = file, \n",
    "                    processor = processor,\n",
    "                    model = whisper_model,\n",
    "                    temperature=args.temperature,\n",
    "                    no_speech_threshold=args.no_speech_threshold,\n",
    "                    logprob_threshold=args.logprob_threshold,\n",
    "                )\n",
    "            \n",
    "            with open(output_file + '.json', 'w') as f:\n",
    "                json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: \", e)\n",
    "            print(\"Please check: \", file)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    model =  \"vinai/PhoWhisper-small\"\n",
    "    prompt = 'lời nhạc: '\n",
    "    language = 'vi'\n",
    "    input_dir = '/home/anh/Documents/vietnamese-song-scraping/out/validation-audio-100-demuc'\n",
    "    output_dir = '/home/anh/Documents/vietnamese-song-scraping/out/PhoWhisper-small/validation-audio-100-demuc_nospeech-noremove'\n",
    "    n_shard = 1\n",
    "    shard_rank = 0\n",
    "    threshold = 0\n",
    "    debug = False\n",
    "    top_n_sample = 2\n",
    "    no_speech_threshold = None # 0.6\n",
    "    logprob_threshold = None #-1.0\n",
    "    temperature = (0.4,0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ADDED PhoWhispers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vinai/PhoWhisper-small\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "100%|██████████| 100/100 [27:35<00:00, 16.56s/it]\n"
     ]
    }
   ],
   "source": [
    "transcribe_and_save(model, processor, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
